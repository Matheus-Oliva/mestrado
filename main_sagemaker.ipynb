{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3a1abd-9172-47f0-aa05-4a6a237a3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretrainedmodels\n",
      "  Using cached pretrainedmodels-0.7.4-py3-none-any.whl\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (2.0.0.post101)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (0.15.2a0+072ec57)\n",
      "Collecting munch (from pretrainedmodels)\n",
      "  Using cached munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pretrainedmodels) (4.66.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->pretrainedmodels) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->pretrainedmodels) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->pretrainedmodels) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->pretrainedmodels) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->pretrainedmodels) (1.3.0)\n",
      "Using cached munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Installing collected packages: munch, pretrainedmodels\n",
      "Successfully installed munch-4.0.0 pretrainedmodels-0.7.4\n",
      "Collecting SimpleITK\n",
      "  Using cached SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Using cached SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
      "Installing collected packages: SimpleITK\n",
      "Successfully installed SimpleITK-2.3.1\n",
      "Collecting tensorboardX\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (23.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (4.21.12)\n",
      "Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.6.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pretrainedmodels\n",
    "!pip install SimpleITK\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5a6eae-a83a-41ba-9923-cf2588bb04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "import os\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "import sys\n",
    "import time\n",
    "import functools\n",
    "from os.path import join, exists\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import logging.handlers\n",
    "import torch\n",
    "import ssl\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "pblog = None\n",
    "\n",
    "\n",
    "def get_pblog(*args, **kwargs):\n",
    "    global pblog\n",
    "    if pblog is None:\n",
    "        pblog = ProgressBarLog(*args, **kwargs)\n",
    "    return pblog\n",
    "\n",
    "def log_decorator(call_fn):\n",
    "    @functools.wraps(call_fn)\n",
    "    def log(self, *args, move=False):\n",
    "        if self.current < self.total:\n",
    "            sys.stdout.write(' ' * (self.width + 26) + '\\r')\n",
    "            sys.stdout.flush()\n",
    "        call_fn(self, *args)\n",
    "        if move:\n",
    "            self._current += 1\n",
    "            temp = datetime.now()\n",
    "            delta = temp - self.last_time\n",
    "            self.last_time = temp\n",
    "            temp = temp + delta * (self.total - self.current)\n",
    "            self.ok_time = str(temp).split('.')[0]\n",
    "        if self.current < self.total:\n",
    "            progress = int(self.width * self.current / self.total)\n",
    "            temp = '{:2}%][{}]\\r'.format(int(100 * self.current / self.total),\n",
    "                                         self.ok_time)\n",
    "            sys.stdout.write('[' + '=' * progress + '>' + '-' * (\n",
    "                    self.width - progress - 1) + temp)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "class ProgressBarLog:\n",
    "    def __init__(self, total=50, width=76, current=0, logger=None):\n",
    "        self.width = width - 26\n",
    "        self.total = total\n",
    "        self._current = current\n",
    "        if logger is None:\n",
    "            log_path = join(config['log_dir'], config['cmd'])\n",
    "            desc = '{}_{}_{}_{}'. \\\n",
    "                format(config['dataset'], config['model'], config['action'], config['desc'])\n",
    "            self.logger = gen_logger(gen_t_name(log_path, desc, '.log'))\n",
    "        else:\n",
    "            self.logger = logger\n",
    "        self.last_time = datetime.now()\n",
    "        self.ok_time = None\n",
    "        self.pb_last_time = time.time()\n",
    "        self.pb_begin_time = self.pb_last_time\n",
    "\n",
    "    @property\n",
    "    def current(self):\n",
    "        return self._current\n",
    "\n",
    "    @current.setter\n",
    "    def current(self, value):\n",
    "        if not isinstance(value, int):\n",
    "            raise ValueError\n",
    "        if value < 0 or value > self.total:\n",
    "            raise ValueError\n",
    "        self._current = value\n",
    "\n",
    "    @log_decorator\n",
    "    def debug(self, msg):\n",
    "        self.logger.debug(msg)\n",
    "\n",
    "    @log_decorator\n",
    "    def info(self, msg):\n",
    "        self.logger.info(msg)\n",
    "\n",
    "    @log_decorator\n",
    "    def warning(self, msg):\n",
    "        self.logger.warning(msg)\n",
    "\n",
    "    @log_decorator\n",
    "    def error(self, msg):\n",
    "        self.logger.error(msg)\n",
    "\n",
    "    @log_decorator\n",
    "    def exception(self, msg):\n",
    "        self.logger.exception(msg)\n",
    "\n",
    "    @log_decorator\n",
    "    def print(self, *args):\n",
    "        print(*args)\n",
    "\n",
    "    @log_decorator\n",
    "    def refresh(self):\n",
    "        pass\n",
    "\n",
    "    def pb(self, current, total, msg):\n",
    "        TOTAL_BAR_LENGTH = 45.\n",
    "        # _, term_width = os.popen('stty size', 'r').read().split()\n",
    "        term_width = 94\n",
    "        if current == 0:\n",
    "            self.pb_begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "        cur_len = int(TOTAL_BAR_LENGTH * current / total)\n",
    "        rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "        sys.stdout.write(' [')\n",
    "        for _ in range(cur_len):\n",
    "            sys.stdout.write('=')\n",
    "        sys.stdout.write('>')\n",
    "        for _ in range(rest_len):\n",
    "            sys.stdout.write('.')\n",
    "        sys.stdout.write(']')\n",
    "\n",
    "        cur_time = time.time()\n",
    "        self.pb_last_time = cur_time\n",
    "        tot_time = cur_time - self.pb_begin_time\n",
    "\n",
    "        L = []\n",
    "        L.append('Tot: %s' % format_time(tot_time))\n",
    "        if msg:\n",
    "            L.append(' | ' + msg)\n",
    "\n",
    "        msg = ''.join(L)\n",
    "        sys.stdout.write(msg)\n",
    "        for i in range(term_width - int(TOTAL_BAR_LENGTH) - len(msg) - 3):\n",
    "            sys.stdout.write(' ')\n",
    "\n",
    "        # Go back to the center of the bar.\n",
    "        for i in range(term_width - int(TOTAL_BAR_LENGTH / 2) + 2):\n",
    "            sys.stdout.write('\\b')\n",
    "        sys.stdout.write(' %d/%d ' % (current + 1, total))\n",
    "\n",
    "        if current < total - 1:\n",
    "            sys.stdout.write('\\r')\n",
    "        else:\n",
    "            sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "def gen_logger(file_path, log_name=None):\n",
    "    if log_name is None:\n",
    "        log_name = config['log_name']\n",
    "    cmd_fmt = '[%(asctime)s] @%(name)s %(levelname)-8s%(message)s'\n",
    "    cmd_datefmt = '%Y-%m-%d %H:%M:%S'\n",
    "    formatter = ColoredFormatter(cmd_fmt, cmd_datefmt)\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "    file_handler.formatter = formatter\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.formatter = formatter\n",
    "    logger = logging.getLogger(log_name)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def gen_t_name(base_dir, desc, ext):\n",
    "    check_mkdir(base_dir)\n",
    "    while True:\n",
    "        dt = datetime.now()\n",
    "        temp = join(base_dir, desc + dt.strftime('_%y%m%d_%H%M%S') + ext)\n",
    "        if exists(temp):\n",
    "            time.sleep(0.000001)\n",
    "        else:\n",
    "            break\n",
    "    return temp\n",
    "\n",
    "\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    '''A colorful formatter.'''\n",
    "\n",
    "    def __init__(self, fmt=None, datefmt=None):\n",
    "        logging.Formatter.__init__(self, fmt, datefmt)\n",
    "\n",
    "    def format(self, record):\n",
    "        # Color escape string\n",
    "        COLOR_RED = '\\033[1;31m'\n",
    "        COLOR_GREEN = '\\033[1;32m'\n",
    "        COLOR_YELLOW = '\\033[1;33m'\n",
    "        COLOR_BLUE = '\\033[1;34m'\n",
    "        COLOR_PURPLE = '\\033[1;35m'\n",
    "        COLOR_CYAN = '\\033[1;36m'\n",
    "        COLOR_GRAY = '\\033[1;37m'\n",
    "        COLOR_WHITE = '\\033[1;38m'\n",
    "        COLOR_RESET = '\\033[1;0m'\n",
    "        # Define log color\n",
    "        LOG_COLORS = {\n",
    "            'DEBUG': COLOR_BLUE + '%s' + COLOR_RESET,\n",
    "            'INFO': COLOR_GREEN + '%s' + COLOR_RESET,\n",
    "            'WARNING': COLOR_YELLOW + '%s' + COLOR_RESET,\n",
    "            'ERROR': COLOR_RED + '%s' + COLOR_RESET,\n",
    "            'CRITICAL': COLOR_RED + '%s' + COLOR_RESET,\n",
    "            'EXCEPTION': COLOR_RED + '%s' + COLOR_RESET,\n",
    "        }\n",
    "        level_name = record.levelname\n",
    "        msg = logging.Formatter.format(self, record)\n",
    "        return LOG_COLORS.get(level_name, '%s') % msg\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600 / 24)\n",
    "    seconds = seconds - days * 3600 * 24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours * 3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes * 60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds * 1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54758ff2-579a-4973-8223-2f8085a05403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Se_resnet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Se_resnet, self).__init__()\n",
    "        self.features = pretrainedmodels.se_resnet152(num_classes=1000, pretrained='imagenet')\n",
    "        self.features.last_linear = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e20cc5-6f4c-48b5-84ea-2ec641d8c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GallbladderDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.frame = pd.read_csv(csv_file, encoding='utf-8', header=None)\n",
    "        self.root_dir = root_dir\n",
    "        # print('csv_file source----->', csv_file)\n",
    "        # print('root_dir source----->', root_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.frame.iloc[idx, 0])\n",
    "        img_name = os.path.basename(img_path)\n",
    "        # print(img_name)\n",
    "        _, extension = os.path.splitext(self.frame.iloc[idx, 0])\n",
    "        # print(extension)\n",
    "        image = self.image_loader(img_path, extension)\n",
    "        # print(image)\n",
    "        label = int(self.frame.iloc[idx, 1])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        sample = {'image': image, 'label': label, 'img_name': img_name}\n",
    "        return sample\n",
    "\n",
    "    def image_loader(self, img_name, extension):\n",
    "        if extension == '.JPG':\n",
    "            # print('读取jpg')\n",
    "            return self.read_jpg(img_name)\n",
    "        elif extension == '.jpg':\n",
    "            # print('读取jpg')\n",
    "            return self.read_jpg(img_name)\n",
    "        elif extension == '.DCM':\n",
    "            # print('读取dcm')\n",
    "            return self.read_dcm(img_name)\n",
    "        elif extension == '.dcm':\n",
    "            # print('读取dcm')\n",
    "            return self.read_dcm(img_name)\n",
    "        elif extension == '.Bmp':\n",
    "            # print('读取Bmp')\n",
    "            return self.read_bmp(img_name)\n",
    "        elif extension == '.png':\n",
    "            return self.read_png(img_name)\n",
    "\n",
    "    def read_jpg(self, img_name):\n",
    "        return Image.open(img_name).convert('RGB')\n",
    "\n",
    "    def read_dcm(self, img_name):\n",
    "        ds = sitk.ReadImage(img_name)\n",
    "        img_array = sitk.GetArrayFromImage(ds)\n",
    "        img_bitmap = Image.fromarray(img_array[0])\n",
    "        return img_bitmap\n",
    "\n",
    "    def read_bmp(self, img_name):\n",
    "        return Image.open(img_name)\n",
    "\n",
    "    def read_png(self, img_name):\n",
    "        return Image.open(img_name)\n",
    "\n",
    "\n",
    "def call_gallbladder_dataset():\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = GallbladderDataset(\n",
    "        csv_file='training_data/label.csv',\n",
    "        root_dir=\"training_data\",\n",
    "        transform=tf\n",
    "    )\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=int(len(dataset) * 0.1), shuffle=False, num_workers=2)\n",
    "    for item in dataloader:\n",
    "        images = item['image']\n",
    "        images = images.numpy()\n",
    "        mean = np.mean(images, axis=(0, 2, 3))\n",
    "        std = np.std(images, axis=(0, 2, 3))\n",
    "        break\n",
    "    print(mean, std)\n",
    "class BaseLoader:\n",
    "    def __init__(self, args):\n",
    "        # special params\n",
    "        self.num_workers = 1\n",
    "        if 'batch_size' in args:\n",
    "            self.batch_size = 16\n",
    "\n",
    "        # custom properties\n",
    "        self._dataset_train = None\n",
    "        self._dataset_eval = None\n",
    "        self._dataset_test = None\n",
    "        self._dataset_norm = None\n",
    "        self._dataset_attack = None\n",
    "        self.dataloader_train = None\n",
    "        self.dataloader_eval = None\n",
    "        self.dataloader_test = None\n",
    "\n",
    "    @property\n",
    "    def dataset_train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def dataset_eval(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def dataset_test(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def dataset_norm(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.dataloader_train is None:\n",
    "            self.dataloader_train = Data.DataLoader(self.dataset_train,\n",
    "                                                    batch_size=self.batch_size,\n",
    "                                                    shuffle=True,\n",
    "                                                    num_workers=self.num_workers,\n",
    "                                                    pin_memory=True)\n",
    "        return self.dataloader_train\n",
    "\n",
    "    @property\n",
    "    def eval(self):\n",
    "        if self.dataloader_eval is None:\n",
    "            self.dataloader_eval = Data.DataLoader(self.dataset_eval,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   num_workers=self.num_workers,\n",
    "                                                   pin_memory=True)\n",
    "        return self.dataloader_eval\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.dataloader_test is None:\n",
    "            self.dataloader_test = Data.DataLoader(self.dataset_test,\n",
    "                                                   batch_size=self.batch_size,\n",
    "                                                   num_workers=self.num_workers,\n",
    "                                                   pin_memory=True)\n",
    "        return self.dataloader_test\n",
    "\n",
    "    def cal_norm(self, n=1):\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def random_sample_base(base_dir, transform, size):\n",
    "        classes = os.listdir(base_dir)\n",
    "        classes.sort()\n",
    "        images = []\n",
    "        for c in classes:\n",
    "            folder = join(base_dir, c)\n",
    "            for file_name in np.random.choice(os.listdir(folder), size, False):\n",
    "                img = join(folder, file_name)\n",
    "                images.append(transform(Image.open(img)))\n",
    "        return classes, torch.stack(images)\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(args):\n",
    "    if args['dataset'] == 'Gallbladder':\n",
    "        return Gallbladder(args)\n",
    "    else:\n",
    "        raise ValueError('No dataset: {}'.format(args['dataset']))\n",
    "\n",
    "\n",
    "class Gallbladder(BaseLoader):\n",
    "    # contain empty\n",
    "    mean = [0.359, 0.361, 0.379]\n",
    "    std = [0.190, 0.190, 0.199]\n",
    "    # except empty\n",
    "    # mean = [0.359, 0.361, 0.380]\n",
    "    # std = [0.191, 0.190, 0.200]\n",
    "\n",
    "    num_classes = 2\n",
    "    class_names = ('Biliary atresia', 'Non-biliary atresia')\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(Gallbladder, self).__init__(args)\n",
    "        self.train_dir = 'training_data'\n",
    "        self.test_dir = 'test_data'\n",
    "        self.train_csv = 'training_data/label.csv'\n",
    "        self.test_csv = 'test_data/label.csv'\n",
    "        self.mean = Gallbladder.mean\n",
    "        self.std = Gallbladder.std\n",
    "        self.img_size = args['img_size']\n",
    "        self.convert = transforms.Grayscale(3)\n",
    "\n",
    "    @property\n",
    "    def dataset_train(self):\n",
    "        if self._dataset_train is None:\n",
    "            tf = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.img_size, scale=(0.8, 1)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(self.mean, self.std)\n",
    "            ])\n",
    "            self._dataset_train = GallbladderDataset(csv_file= self.train_csv,\n",
    "                                                                         root_dir=self.train_dir,\n",
    "                                                                         transform=tf)\n",
    "        return self._dataset_train\n",
    "\n",
    "    @property\n",
    "    def dataset_eval(self):\n",
    "        if self._dataset_eval is None:\n",
    "            tf = transforms.Compose([\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                self.gray_to_rgb,\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(self.mean, self.std)\n",
    "            ])\n",
    "            self._dataset_eval = GallbladderDataset(csv_file=self.test_csv,\n",
    "                                                                        root_dir=self.test_dir,\n",
    "                                                                        transform=tf)\n",
    "        return self._dataset_eval\n",
    "\n",
    "    def gray_to_rgb(self, img):\n",
    "        size = np.array(img).transpose().shape\n",
    "        # print(size)\n",
    "        if size[0] != 3:\n",
    "            img = self.convert(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "def call_dataloader():\n",
    "    args = {'dataset': 'Gallbladder',\n",
    "            'Gallbladder_train_dir': '/home/cccc/Desktop/share/deeplearning_project/pnasnet/data/DataSets/final_train',\n",
    "            'Gallbladder_test_dir': '/home/cccc/Desktop/share/deeplearning_project/pnasnet/data/DataSets/final_train',\n",
    "            'train_csv': './label.csv',\n",
    "            'test_csv': './label.csv',\n",
    "            'num_workers': 4,\n",
    "            'batch_size': 16,\n",
    "            'img_size': 331}  # pnasnet input size=331\n",
    "    dl = get_dataloader(args).train\n",
    "    print(len(dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c90afd6-8701-43a3-956e-854ace454e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAction:\n",
    "    loss_legend = ['| loss: {:0<10.8f}']\n",
    "    eval_on_train = True\n",
    "    eval_legend = ['| acc: {:0<5.3f}%']\n",
    "    eval_personal_legend = ['| sensitivity: {:0<5.3f}%', '| specitivity: {:0<5.3f}%']\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_logits(x, net):\n",
    "        return net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_loss(y, y_hat, weight):\n",
    "        #weigth = torch.tensor([weight]).cuda()\n",
    "        weigth = torch.tensor([weight])\n",
    "        loss = F.cross_entropy(y_hat, y, weight=weigth.reshape((2,)))\n",
    "        return loss,\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_eval(y, y_hat):\n",
    "        count_right = np.empty(1, np.float32)\n",
    "        count_sum = np.empty(1, np.float32)\n",
    "        y_hat = y_hat.argmax(1)\n",
    "        count_right[0] = (y_hat == y).sum().item()\n",
    "        count_sum[0] = y.size(0)\n",
    "        return 100 * count_right, count_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def update_opt(epoch, net, opt_type, lr=1e-2, lr_epoch=35):\n",
    "        decay = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        if epoch % lr_epoch == 0:\n",
    "            times = int(epoch / lr_epoch)\n",
    "            times = len(decay) - 1 if times >= len(decay) else times\n",
    "            if opt_type == 'sgd':\n",
    "                return torch.optim.SGD(net.parameters(), lr=lr * decay[times],\n",
    "                                       momentum=0.9,\n",
    "                                       weight_decay=5e-4)\n",
    "            elif opt_type == 'adam':\n",
    "                return torch.optim.Adam(net.parameters(), lr=lr * decay[times])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(ism, model, path, *args):\n",
    "        ''''\n",
    "        ism, model, path, msg, pblog, acc, epoch\n",
    "        '''\n",
    "        acc, epoch = args\n",
    "        if ism:\n",
    "            state_dict = model.module.state_dict()\n",
    "        else:\n",
    "            state_dict = model.state_dict()\n",
    "        state = {\n",
    "            'net': state_dict,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch}\n",
    "        torch.save(state, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_graph(model, img_size, tblog, pblog):\n",
    "        #dummyInput = torch.randn([1, 3, img_size, img_size]).cuda()\n",
    "        dummyInput = torch.randn([1, 3, img_size, img_size])\n",
    "        tblog.add_graph(model, dummyInput)\n",
    "        pblog.debug('Graph saved')\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_scalars(metric, metric_legend, msg, pblog):\n",
    "        scalars = dict()\n",
    "        for n, s in zip(metric, metric_legend):\n",
    "            msg += s.format(n)\n",
    "            scalars[s.split(':')[0][2:]] = n\n",
    "        pblog.info(msg)\n",
    "        return scalars\n",
    "\n",
    "    # @staticmethod\n",
    "    # def log_confusion_matrix(labels, predictions, class_names):\n",
    "    #     cm = confusion_matrix(labels, predictions)\n",
    "    #     # normalise confusion matrix for diff sized groups\n",
    "    #     cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #     cm_image = plot_confusion_matrix(cm_norm, class_names)\n",
    "    #     return cm_image\n",
    "\n",
    "\n",
    "def get_action(args):\n",
    "    action = args['action']\n",
    "    if action == 'base':\n",
    "        return BaseAction()\n",
    "    else:\n",
    "        raise ValueError('No action: {}'.format(action))\n",
    "def check_mkdir(dir_name):\n",
    "    if not exists(dir_name):\n",
    "        os.makedirs(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04666cc6-b140-4098-9854-d83ac8c6bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, args):\n",
    "        # cuda setting\n",
    "        #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args['cuda']\n",
    "\n",
    "        # dir setting\n",
    "        self.model_dir = args['model_dir']\n",
    "        self.best_model_dir = args['best_model_dir']\n",
    "        self.step_model_dir = args['step_model_dir']\n",
    "        check_mkdir(self.model_dir)\n",
    "        check_mkdir(self.best_model_dir)\n",
    "\n",
    "        # dataset setting\n",
    "        self.dataloader = get_dataloader(args)\n",
    "        self.no_eval = args['no_eval']\n",
    "        self.personal_eval = args['personal_eval']\n",
    "        self.img_size = args['img_size']\n",
    "        args['mean'] = self.dataloader.mean\n",
    "        args['std'] = self.dataloader.std\n",
    "        args['num_classes'] = self.dataloader.num_classes\n",
    "\n",
    "        # basic setting\n",
    "        self.opt_type = args['optimizer']\n",
    "        self.lr = args['lr']\n",
    "        self.lr_epoch = args['lr_epoch']\n",
    "        self.epoch = args['epoch']\n",
    "        self.weight = args['weight']\n",
    "        self.eval_best = 0\n",
    "        self.eval_best_epoch = 0\n",
    "        self.save_cm = args['save_cm']  # save confusion matrix\n",
    "\n",
    "        # model name config\n",
    "        self.model_desc = '{}_{}_{}_{}'. \\\n",
    "            format(args['dataset'], args['model'], args['action'], args['desc'])\n",
    "        self.model_pkl = self.model_desc + '.ckpt'\n",
    "\n",
    "        # logger setup\n",
    "        self.pblog = get_pblog()\n",
    "        self.pblog.total = self.epoch\n",
    "        self.tblog = SummaryWriter(join(args['tb_dir'], self.model_desc))\n",
    "\n",
    "        # model setup\n",
    "        self.action = get_action(args)\n",
    "        self.model = Se_resnet(2)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'tb_log'):\n",
    "            self.tblog.close()\n",
    "\n",
    "    def train(self):\n",
    "        self.pblog.info(self.model_desc)\n",
    "        optimizer = None\n",
    "        for epoch in range(self.epoch):\n",
    "            # get optimizer\n",
    "            temp = self.action.update_opt(epoch, self.model, self.opt_type,\n",
    "                                          self.lr, self.lr_epoch)\n",
    "            if temp is not None:\n",
    "                optimizer = temp\n",
    "\n",
    "            self.model.train()\n",
    "            loss_l = []\n",
    "            loss_n = []\n",
    "            dl_len = len(self.dataloader.train)\n",
    "            ll = len(self.action.eval_legend)\n",
    "            c_right = np.zeros(ll, np.float32)\n",
    "            c_sum = np.zeros(ll, np.float32)\n",
    "            main_loss = 0\n",
    "            for idx, item in enumerate(self.dataloader.train):\n",
    "                tx, ty = item['image'], item['label']\n",
    "                #tx, ty = tx.cuda(non_blocking=True), ty.cuda(non_blocking=True)\n",
    "                # get network output logits\n",
    "                logits = self.action.cal_logits(tx, self.model)\n",
    "                # cal loss\n",
    "                loss = self.action.cal_loss(ty, logits, self.weight)\n",
    "                # cal acc\n",
    "                right_e, sum_e = self.action.cal_eval(ty, logits)\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss[0].backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                c_right += right_e\n",
    "                c_sum += sum_e\n",
    "                loss_l.append([ii.item() for ii in loss])\n",
    "                loss_n.append(ty.size(0))\n",
    "                main_loss += loss[0].item()\n",
    "                self.pblog.pb(idx, dl_len, 'Loss: %.5f | Acc: %.3f%%' % (\n",
    "                    main_loss / (idx + 1), c_right / c_sum))\n",
    "            loss_l = np.array(loss_l).T\n",
    "            loss_n = np.array(loss_n)\n",
    "            loss = (loss_l * loss_n).sum(axis=1) / loss_n.sum()\n",
    "            c_res = c_right / c_sum\n",
    "\n",
    "            msg = 'Epoch: {:>3}'.format(epoch)\n",
    "            loss_scalars = self.action.cal_scalars(loss,\n",
    "                                                   self.action.loss_legend, msg,\n",
    "                                                   self.pblog)\n",
    "            self.tblog.add_scalars('loss', loss_scalars, epoch)\n",
    "\n",
    "            msg = 'train->   '\n",
    "            acc_scalars = self.action.cal_scalars(c_res,\n",
    "                                                  self.action.eval_legend, msg,\n",
    "                                                  self.pblog)\n",
    "            self.tblog.add_scalars('eval/train', acc_scalars, epoch)\n",
    "\n",
    "            if not self.no_eval:\n",
    "                if not self.personal_eval:\n",
    "                    with torch.no_grad():\n",
    "                        self.eval(epoch)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        self.eval_personal(epoch)\n",
    "\n",
    "        path = os.path.join(self.model_dir, self.model_desc)\n",
    "        self.action.save_model(self.ism, self.model, path, self.eval_best,\n",
    "                               self.eval_best_epoch)\n",
    "        self.pblog.debug('Training completed, save the last epoch model')\n",
    "        temp = 'Result, Best: {:.2f}%, Epoch: {}'.format(self.eval_best,\n",
    "                                                         self.eval_best_epoch)\n",
    "        self.tblog.add_text('best', temp, self.epoch)\n",
    "        self.pblog.info(temp)\n",
    "\n",
    "    def eval(self, epoch):\n",
    "        self.model.eval()\n",
    "        ll = len(self.action.eval_legend)\n",
    "        c_right = np.zeros(ll, np.float32)\n",
    "        c_sum = np.zeros(ll, np.float32)\n",
    "        dl_len = len(self.dataloader.eval)\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for idx, item in enumerate(self.dataloader.eval):\n",
    "            x, y = item['image'], item['label']\n",
    "            #x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "            logits = self.action.cal_logits(x, self.model)\n",
    "            right_e, sum_e = self.action.cal_eval(y, logits)\n",
    "            c_right += right_e\n",
    "            c_sum += sum_e\n",
    "            labels.extend(y.cpu().data)\n",
    "            predictions.extend(logits.argmax(1).cpu().data)\n",
    "            self.pblog.pb(idx, dl_len, 'Acc: %.3f %%' % (c_right / c_sum))\n",
    "        msg = 'eval->    '\n",
    "        c_res = c_right / c_sum\n",
    "        acc_scalars = self.action.cal_scalars(c_res, self.action.eval_legend,\n",
    "                                              msg, self.pblog)\n",
    "        self.tblog.add_scalars('eval/eval', acc_scalars, epoch)\n",
    "\n",
    "        # if self.save_cm:\n",
    "        #     cm_figure = self.action.log_confusion_matrix(labels, predictions,\n",
    "        #                                                  self.dataloader.class_names)\n",
    "        #     self.tblog.add_figure('Confusion Matrix', cm_figure, epoch)\n",
    "\n",
    "        if c_res[0] > self.eval_best and epoch > 30:\n",
    "            self.eval_best_epoch = epoch\n",
    "            self.eval_best = c_res[0]\n",
    "            path = os.path.join(self.best_model_dir, 'Best_' + self.model_desc)\n",
    "            self.action.save_model(self.ism, self.model, path, self.eval_best,\n",
    "                                   self.eval_best_epoch)\n",
    "            self.pblog.debug('Update the best model')\n",
    "\n",
    "    def eval_personal(self, epoch):\n",
    "        self.model.eval()\n",
    "        ll = len(self.action.eval_legend)\n",
    "        c_right = np.zeros(ll, np.float32)\n",
    "        c_sum = np.zeros(ll, np.float32)\n",
    "        dl_len = len(self.dataloader.eval)\n",
    "\n",
    "        labels = []\n",
    "        predictions = []\n",
    "        preindex = None\n",
    "        prelabel = None\n",
    "        personal_vote = [0. for i in range(2)]\n",
    "        class_correct = list(0. for i in range(2))\n",
    "        class_total = list(0. for i in range(2))\n",
    "        for idx, item in enumerate(self.dataloader.eval):\n",
    "            x, y, img_names = item['image'], item['label'], item['img_name']\n",
    "            #x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "            logits = self.action.cal_logits(x, self.model)\n",
    "            _, prediction = torch.max(logits.data, 1)\n",
    "            for i, name in enumerate(img_names):\n",
    "                index, *_ = name.split('_')\n",
    "                # init pre\n",
    "                if preindex is None:\n",
    "                    preindex = index\n",
    "                if prelabel is None:\n",
    "                    prelabel = y[0]\n",
    "\n",
    "                if index != preindex:\n",
    "                    if personal_vote[0] >= personal_vote[1]:\n",
    "                        predictions.append(0)\n",
    "                        if prelabel == 0:\n",
    "                            class_correct[0] += 1\n",
    "                    else:\n",
    "                        predictions.append(1)\n",
    "                        if prelabel == 1:\n",
    "                            class_correct[1] += 1\n",
    "                    labels.append(prelabel.item())\n",
    "                    class_total[prelabel] += 1\n",
    "                    personal_vote = [0. for i in range(2)]\n",
    "                    preindex = index\n",
    "                    prelabel = y[i]\n",
    "                    personal_vote[prediction[i]] += 1\n",
    "\n",
    "                else:\n",
    "                    personal_vote[prediction[i]] += 1\n",
    "\n",
    "            self.pblog.pb(idx, dl_len, 'Sen: %.3f %%  | Spe: %.3f %%' % (\n",
    "                100 * class_correct[0] / (class_total[0] + 1e-6), 100 * class_correct[1] / (class_total[1] + 1e-6)))\n",
    "\n",
    "        # deal the last patient\n",
    "        if personal_vote[0] >= personal_vote[1]:\n",
    "            predictions.append(0)\n",
    "            if prelabel == 0:\n",
    "                class_correct[0] += 1\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "            if prelabel == 1:\n",
    "                class_correct[1] += 1\n",
    "\n",
    "        labels.append(prelabel.item())\n",
    "        class_total[prelabel] += 1\n",
    "\n",
    "        msg = 'eval->    '\n",
    "        c_res = [100 * class_correct[0] / class_total[0], 100 * class_correct[1] / class_total[1]]\n",
    "        acc_scalars = self.action.cal_scalars(c_res, self.action.eval_personal_legend,\n",
    "                                              msg, self.pblog)\n",
    "        self.tblog.add_scalars('eval/eval', acc_scalars, epoch)\n",
    "\n",
    "        # if self.save_cm:\n",
    "        #     cm_figure = self.action.log_confusion_matrix(labels, predictions,\n",
    "        #                                                  self.dataloader.class_names)\n",
    "        #     self.tblog.add_figure('Confusion Matrix', cm_figure, epoch)\n",
    "\n",
    "        if c_res[0] > self.eval_best and epoch > 30 and class_correct[1] / class_total[1] >= 0.85:\n",
    "            self.eval_best_epoch = epoch\n",
    "            self.eval_best = c_res[0]\n",
    "            path = os.path.join(self.best_model_dir, 'Best_' + self.model_desc)\n",
    "            self.action.save_model(self.ism, self.model, path, self.eval_best,\n",
    "                                   self.eval_best_epoch)\n",
    "            self.pblog.debug('Update the best model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a2bf08-d646-4f12-adee-1b756fcdedad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2024-04-04 15:06:21] @Modelo base INFO    Gallbladder_Se_resnet_base_\u001b[1;0m\n",
      "\u001b[1;32m[2024-04-04 15:06:21] @Modelo base INFO    Gallbladder_Se_resnet_base_\u001b[1;0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>............................................]Tot: 0ms | Loss: 0.69673 | Acc: 81.250%      1/232 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/1177612430.py:89: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.pblog.pb(idx, dl_len, 'Loss: %.5f | Acc: %.3f%%' % (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================================>]Tot: 29m49s | Loss: 0.55854 | Acc: 66.019%   232/232 \n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2024-04-04 15:36:20] @Modelo base INFO    Epoch:   0| loss: 0.55861174\u001b[1;0m\n",
      "\u001b[1;32m[2024-04-04 15:36:20] @Modelo base INFO    Epoch:   0| loss: 0.55861174\u001b[1;0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2024-04-04 15:36:20] @Modelo base INFO    train->   | acc: 66.019%\u001b[1;0m\n",
      "\u001b[1;32m[2024-04-04 15:36:20] @Modelo base INFO    train->   | acc: 66.019%\u001b[1;0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>............................................]Tot: 0ms | Acc: 62.500 %                     1/53 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/1177612430.py:142: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.pblog.pb(idx, dl_len, 'Acc: %.3f %%' % (c_right / c_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [============================================>]Tot: 1m59s | Acc: 60.642 %                   53/53 \n",
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[2024-04-04 15:38:23] @Modelo base INFO    eval->    | acc: 60.642%\u001b[1;0m\n",
      "\u001b[1;32m[2024-04-04 15:38:23] @Modelo base INFO    eval->    | acc: 60.642%\u001b[1;0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [>............................................]Tot: 0ms | Loss: 0.52100 | Acc: 68.750%      1/232 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/1177612430.py:89: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.pblog.pb(idx, dl_len, 'Loss: %.5f | Acc: %.3f%%' % (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [===================================>.........]Tot: 23m42s | Loss: 0.40099 | Acc: 78.649%   185/232 \r"
     ]
    }
   ],
   "source": [
    "data_dir = ''\n",
    "config = {\n",
    "    'optimizer': 'sgd',\n",
    "    'dataset': 'ImageNet100',\n",
    "    'img_size': 224,\n",
    "    'model': 'Se_resnet',\n",
    "    'dataset': 'Gallbladder',\n",
    "    'lr': 0.01,\n",
    "    'lr_epoch': 35,\n",
    "    'epoch': 100,\n",
    "    'pre_train': False,\n",
    "    'batch_size': 64,\n",
    "    'weight': [5.0, 1.0],\n",
    "    'save_cm': False,\n",
    "    'log_dir': join(data_dir, 'log'),\n",
    "    'loss_dir': join(data_dir, 'loss'),\n",
    "    'model_dir': join(data_dir, 'model'),\n",
    "    'best_model_dir': join(data_dir, 'best_model'),\n",
    "    'step_model_dir': join(data_dir, 'step_temp'),\n",
    "    'tb_dir': join(data_dir, 'tb'),\n",
    "    'action': 'base',\n",
    "    'desc': '',\n",
    "    'cmd': 'train',\n",
    "    'log_name': 'Modelo base',\n",
    "    'cuda': '0',\n",
    "    'num_workers': 2,\n",
    "    'no_eval': False,\n",
    "    'personal_eval': False\n",
    "}\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "pblog = get_pblog(total = 100)\n",
    "Trainer(config).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a71e8-6b9a-4bf8-934b-23ac191963f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
